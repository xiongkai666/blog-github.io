## 为什么大模型参数量为7b、14b等⭐
为了适配各种显卡的显存（8GB、16GB）


主要工作：1、完成 Llama3、Qwen2.5/3 等主流模型推理框架的搭建，支持 AWQ、GPTQ 等量化技术，实现精度损失低于 5%，显存占用降低 50%-75%。
2、采用 FlashAttention V1、V2 和 Flash-Decoding 技术优化注意力计算，提升长序列（1-128k tokens）推理速度；采用 cuda graph 优化推理流程，吞吐量提升 3x 倍。
3、通过 TokenAttention 方案实现 KV 缓存的 Token 级显存管理：模型初始化时预分配 KV 缓存并创建 Token Table 记录 Token 位置，处理请求时优先分配连续内存（不足则分配非连续块），KV 缓存显存占用降低 30% 。
4、完成 RMSNorm、RoPE 和 GQA 等高效 CUDA 算子，并实现 KV 线性层融合, Skip 和 RMSNorm 融合等融合算子，相较算子融合前内存访问开销降低 20%。
项目成果：在单卡 A00 GPU上，相比原版 transformers，Llama-3.2-11B 模型加速比最高达 5x 倍。


1、支持KV-Cache等常见优化手段(Continous batching, PageAtention，投机采样)。
2、实现RMSNorm、MatMul、KV-Cache 以及 MHA, FlashAttention等核心算子
3、经过调优，模型推理吞吐量提升了[X]%。推理速度提升至 yy Token/s，显存占用优化至 zz GB。(或者模型的首字时延降低至[X]毫秒，平均时延降至[X]毫秒，相比优化前分别降低了[X]%和[X]%)

项目成果:
1.完成 Llama 2/3.2和Qwen 模型中多个复杂 CUDA 算子的开发与性能优化，涵盖RMSNorm、MatMul、KV-Cache 和 MHA 等，分析并优化了内存管理技术，显著提高模型推理速度和资源利用率。优化后，内存占用峰值降低了[X]% ，性能提供了[Y]%。

2.深入了解并熟练运用多种大型模型推理框架，如lama.cpp、VLLM 及 TensorRT等，对 llama.cpp 框架进行深入研究，熟练掌握其内部实现和优化技巧。这里你可以列举下VLLM的优化技术，比如Multi-step scheduling Chunked prefill, Speculative decoding,PagedAttention, Continuous batching，自己按名词学习一下即可。

3.成功应用 int8 分组量化技术，为模型推理框架在资源受限环境下的高效运行提供了有力支持，为项目拓展了更广泛的应用场景。 这里可以扩展-下AWQ量化技术。



- 支持最新的 `llama3`、`Qwen2.5`、`Qwen3`、`Llava1.5` 模型推理，支持 `top-p` 采样, 支持流式输出。
- 支持 GQA、cuda graph 优化。
- 支持 `flashattention1`、`flashattention2`、 `flashdecoding`(支持 `NopadAttention`)。
- 支持 kv cache 的高效动态管理（`auto tokenattnetion`）。
- 支持算子融合，如：逐元素相乘 `*` 和 `silu` 的融合, k v 线性层融合, `skip` 和 `rmsnorm` 融合。
- 部分自定义算子如：`rmsnorm`、`rope`、`softmax`、`逐元素相乘` 等采用高效 `triton` 内核实现。
- 支持连续批处理优化。
- 支持 AWQ 和 SmoothQuant 量化。
- 相比 transformers, llama3 1B 和 3B 模型加速比最高达 `4x` 倍。

CUDA 图优化解码阶段：运用 CUDA 图对解码阶段进行优化，成功将单次解码时间从 17.2241ms 降至 8.2402ms，实现近 2 倍性能提升，与 vllm 应用 CUDA 图后的效果相近。
替换注意力机制：采用 flashattention 替代原标准注意力机制，尽管 flashattention1 在训练中作用显著，但在短提示词场景下加速效果有限。优化后，批量推理时间从 3152.0476ms 缩短至 2681.3823ms，每秒处理 tokens 数量从 97.71 提升至 114.87。
升级 flashattention 版本：将 flashattention 升级至 2 版本，进一步减少计算量，批量推理时间缩短至 2103.0737ms，tokens 处理速度提升至 146.45 tokens/s。
应用 flashdecoding：在解码阶段使用 flashdecoding，增强注意力计算并行性，充分释放 GPU 算力，使解码阶段批量推理时间降至 1641.4178ms，tokens 处理速度达 187.64 tokens/s。
优化 KV 缓存管理：借鉴 TokenAttention，实现高效动态 KV 缓存管理，解决内存浪费与分配低效问题，解码阶段批量推理时间进一步缩短至 1413.9111ms，tokens 处理速度提升至 217.84 tokens/s。
函数替换与层融合：以 GQA_KV_heads_index 替换 repeat_kv 函数，融合关键和值线性层，简化计算流程。同时，通过算子融合，将残差连接的跳过操作与 rmsnorm 算子整合为 skip_rmsnorm 算子，减少计算开销。
重构优化 MHA 模块：对 MHA 模块进行深度重构，改进 context_attention 和 token_attention 内核，支持 Nopad attention 及 KV 缓存动态分配管理。token_attention 支持直接传递 kv_cache 索引和实际序列长度，减少模块内 concat 和 view 操作；在推理过程中，根据实际提示长度动态分配 kv_cache 索引数量，摒弃预先分配连续 kv_cache 空间的方式，提高内存使用效率与计算性能。
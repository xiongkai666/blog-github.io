---
title: LoRA原理解析和实践
date: 2025-01-23 22:00:38
tags: 
    - LLM
    - LoRA
categories: 大模型后训练
---
# LoRA简介
LoRA的核心思想是，在冻结预训练模型权重后，将可训练的低秩分解矩阵注入到的Transformer架构的每一层中，从而大大减少了在下游任务上的可训练参数量。
## 具体做法
1. 在原模型旁边增加一个旁路，通过低秩分解来模拟参数的更新量$\Delta{W}$；
2. 训练时，原模型固定，只训练矩阵A、B；
3. 推理时，可将BA加到原参数上，不引入额外的推理延迟；
4. 初始化，A采用高斯函数初始化，B初始化为全0，保证训练开始时旁路为0矩阵；
## 优势
1. 存储与计算效率：通过低秩适应（LoRA），可以显著减少所需存储的参数数量，并减少计算需求。
2. 适应性与灵活性: LoRA方法允许模型通过只替换少量特定的矩阵A和B来快速适应新任务，显著提高任务切换的效率。当前任务$W_0+B_1A_1$，将lora部分换成$B_2A_2$，即可实现任务切换。
3. 训练与部署效率：LoRA的简单线性设计允许在不引入推理延迟的情况下，与冻结的权重结合使用，从而提高部署时的操作效率。
# QLoRA 
QLoRA 同时结合了模型量化和 LoRA 参数微调两种方法。QLoRA 针对模型权重（weight）做量化，采用的是对称量化算法。

## 量化部分的创新点：
1. **采用新的 NF（NormalFloat）数据类型**，它是对于正态分布权重而言信息理论上最优的数据类型，同时，NF 类型有助于缓解异常值的影响；
   - int4 的格点分布是均匀的，然而模型的权重通常服从均值为 0 的正态分布，因此格点的分布和数据的分布不一致。
   - NF4 的格点按照正态分布的分位数截取，格点分布两端稀疏，中间密集，格点分布与数据分布一致。这样格点分配的效率就大大增加了，同时精度受损也不会太大。

2. **Double Quant**，对于量化后的 scale 数据做进一步的量化；
   - QLoRA 将每 64 个参数为做一个 block，即 block_size = 64，每个 block 计算一个 Scale。由于量化后的 Scale 通常以 FP32 存储，在 block 数众多的情况下，Scale 占用的显存也不可忽视。因此，QLoRA 对 Scale 进一步量化成 FP8，取 Double Quant 的 block size = 256，因而进一步降低了显存消耗。

   - Double Quant 前，每个参数做量化会需要额外的 32/64 = 0.5 bits 显存；Double Quant 后，每个参数做量化只需要额外的 8/64 + 32 / (64*256) = 0.127 bits 显存。
# 面试题
## 讲一讲lora和qlora ⭐⭐
## lora微调每个超参数的含义和作用 ⭐
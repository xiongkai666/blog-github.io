## 描述一下Transformer

### **Transformer的核心定位**  
Transformer是2017年Google团队在论文《Attention Is All You Need》中提出的深度学习模型，**彻底改变了自然语言处理（NLP）领域的范式**。它完全基于自注意力机制（Self-Attention），替代了此前主流的RNN/LSTM等循环神经网络，解决了循环模型的两大痛点：**并行计算能力弱**（RNN需按序列顺序处理，无法并行）和**长距离依赖捕捉能力有限**（依赖序列长度累积信息，长文本中衰减严重）。目前，几乎所有主流预训练模型（如BERT、GPT、T5等）均以Transformer为基础，成为NLP任务（翻译、问答、文本生成等）的核心架构。


### **Transformer的整体结构**  
Transformer由**Encoder（编码器）** 和**Decoder（解码器）** 两部分组成，整体结构如下：  
- **Encoder**：接收输入序列（如源语言句子），输出一个上下文感知的序列表示（称为“编码向量”）。  
- **Decoder**：接收目标序列（如目标语言句子）和Encoder的输出，生成最终的预测序列（如翻译结果）。  

两者均由**N个相同的层堆叠**而成（论文中N=6），且每个层都包含“残差连接+层归一化”的标准设计，以稳定训练。


### **一、输入处理：词嵌入与位置编码**  
Transformer没有循环结构，无法像RNN那样天然感知序列顺序，因此输入需经过两步预处理：  

#### 1. 词嵌入（Token Embedding）  
将输入文本的每个词（或子词，如BPE分词）转换为固定维度的向量（如512维）。例如，“apple”会被映射为一个512维的向量，用于计算机器可理解的语义表示。  

#### 2. 位置编码（Positional Encoding）  
为了让模型感知词的位置信息，需给每个词的嵌入向量叠加一个“位置编码向量”。论文中采用**正弦余弦函数**计算位置编码：  
- 对于位置`pos`和维度`i`：  
  - 若`i`为偶数：`PE(pos, i) = sin(pos / 10000^(2i/d_model))`  
  - 若`i`为奇数：`PE(pos, i) = cos(pos / 10000^(2i/d_model))`  

  这样设计的好处是：位置差固定时，编码的相对关系保持一致（例如，pos和pos+k的编码差异是固定的），便于模型学习长距离依赖。  
  （后续研究中也常用“可学习的位置编码”，效果类似。）  


### **二、Encoder（编码器）：生成上下文感知的表示**  
Encoder的每个层包含两个核心子层，整体流程为：  
**输入 → 多头自注意力 → 残差+层归一化 → 前馈神经网络 → 残差+层归一化 → 输出**  

#### 1. 多头自注意力机制（Multi-Head Self-Attention）  
这是Transformer的核心，用于建模序列中词与词之间的依赖关系（如“他”指代“小明”，“苹果”和“吃”的关联）。  

- **自注意力（Self-Attention）原理**：  
  对每个词，通过3个线性变换生成3个向量：  
  - Query（Q）：“我要找什么信息”  
  - Key（K）：“我能提供什么信息”  
  - Value（V）：“我提供的信息内容  

  然后计算每个词对其他词的“注意力权重”（即相关性）：  
  1. 计算Q与所有K的点积（衡量匹配度）：`score = Q · K^T`  
  2. 除以`√d_k`（`d_k`是Q/K的维度，如64），避免点积结果过大导致softmax梯度消失  
  3. 通过softmax归一化，得到权重`α = softmax(score / √d_k)`  
  4. 权重与V相乘，得到该词的注意力输出：`output = α · V`  

- **多头（Multi-Head）的作用**：  
  若直接用一个注意力头，模型只能学习一种依赖模式。多头机制将Q/K/V拆分为`h`个并行的子空间（如h=8），每个子空间独立计算自注意力，最后将所有头的输出拼接，再通过线性变换合并：  
  `MultiHead(Q,K,V) = Concat(head_1, ..., head_h) · W^O`  
  这样模型可同时学习多种依赖关系（如语法依赖、语义关联等），提升表达能力。  

#### 2. 前馈神经网络（Feed-Forward Network）  
对多头注意力的输出进行非线性变换，每个词独立处理（无序列依赖）：  
`FFN(x) = max(0, x·W1 + b1) · W2 + b2`  
- 包含两个线性层，中间用ReLU激活（也可用GELU等），作用是增强模型的非线性拟合能力。  

#### 3. 残差连接与层归一化（Residual Connection & Layer Normalization）  
- 残差连接：`x + SubLayer(x)`（将输入与子层输出相加），避免深层网络的梯度消失，让模型更容易训练。  
- 层归一化：对每个样本的特征维度做归一化（均值为0，方差为1），稳定训练过程。  


### **三、Decoder（解码器）：生成目标序列**  
Decoder的每个层包含三个核心子层，整体流程为：  
**输入 → 掩码多头自注意力 → 残差+层归一化 → 编码器-解码器注意力 → 残差+层归一化 → 前馈神经网络 → 残差+层归一化 → 输出**  

#### 1. 掩码多头自注意力（Masked Multi-Head Self-Attention）  
与Encoder的多头自注意力类似，但额外添加了**掩码（Mask）**：在计算注意力权重时，强制让每个词只能“看到”它之前的词（不能关注未来的词）。例如，生成第3个词时，只能用第1、2个词的信息，避免“偷看答案”。  

  掩码通过在softmax前将未来位置的分数设为-∞实现（softmax后权重为0）。  

#### 2. 编码器-解码器注意力（Encoder-Decoder Attention）  
用于结合Encoder的输出（源语言上下文）和解码器的当前状态（目标语言已生成的部分）：  
- Query（Q）来自Decoder的掩码注意力输出（“我现在需要什么信息”）  
- Key（K）和Value（V）来自Encoder的最终输出（“源语言能提供什么信息”）  
  作用是让Decoder在生成每个词时，关注源语言中相关的词（如翻译时，“猫”对应“cat”）。  


### **四、输出层：生成最终预测**  
Decoder的最终输出经过一个线性层（将维度映射到词表大小）和softmax，得到下一个词的概率分布：  
`P(next_token) = softmax(Decoder_output · W + b)`  
- 训练时，用交叉熵损失优化；推理时，通过贪婪搜索、束搜索等生成序列。  


### **Transformer的核心优势**  
1. **并行计算能力强**：  
   不同于RNN按顺序处理序列（无法并行），Transformer的所有词可同时计算注意力和前馈网络，训练速度大幅提升（尤其长序列）。  

2. **长距离依赖建模更优**：  
   RNN需逐步传递信息，长距离依赖会衰减；而自注意力直接计算任意两个词的关联，无论距离远近，权重都是直接学习的。  

3. **迁移学习友好**：  
   基于Transformer的预训练模型（如BERT、GPT）可通过微调适配多种任务，避免重复训练，成为NLP的“基础设施”。  


### **总结**  
Transformer通过**多头自注意力**和**并行计算**，解决了传统序列模型的核心痛点，不仅革新了机器翻译，更推动了NLP从“任务特定模型”向“通用预训练模型”转变。理解其结构（Encoder/Decoder、注意力机制、位置编码等）是掌握现代NLP技术的基础。
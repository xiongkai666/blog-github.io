---
title: BERT面试经验
date: 2025-01-08 17:40:38
tags: BERT
categories: 常见大模型
---
## 为什么BERT要采用Encoder-Only模式
### 1. 核心目标：捕获双向上下文信息，适应自然语言理解任务
- BERT 的核心目标是 基于双向上下文的信息进行预训练，以捕获输入文本中更丰富的语义关系。
- 相较于传统单向语言模型（如基于 RNN 或 Transformer Decoder 的语言模型）只能从左到右或从右到左处理上下文，BERT 的 Encoder 模式通过自注意力机制能够实现双向上下文编码，捕获更丰富的语义关联。
---
### 2. 架构特点：Encoder 模式具有全局自注意力机制，能更好地建模词语的上下文关系
- **全局自注意力机制**是 Transformer Encoder 的核心特性。它允许每个词与输入序列中的所有其他词进行关联，而不是像 RNN 那样只能逐步传递信息或像卷积网络那样局部化建模。
- Encoder 的自注意力机制能够对句子中的长距离依赖进行精确建模。例如，在长句中，主语和谓语可能相隔较远，Encoder 的设计使得模型能够全局建模这种关系。
- Decoder 模式通常会引入**掩码机制（masking）**，只关注当前及之前的单词，避免生成时看到后续的词，从而更适合生成任务（如语言生成或翻译）。而对于 BERT 的目标任务，这种掩码限制会导致模型无法获取完整的上下文信息，因此选择 Encoder 是更优解。
---
### 3. 预训练任务需求：Masked Language Model 和 Next Sentence Prediction 均需要双向信息
- **Masked Language Model (MLM)**：
  - BERT 的预训练任务之一是预测被随机掩盖的词汇的正确词。为了准确预测被掩盖词，模型需要整合其左、右两侧的上下文信息。
  - Encoder 的双向自注意力机制天然适合这种任务，能够为被掩盖词构建全局的上下文语义表示。
- **Next Sentence Prediction (NSP)**：
  - NSP 任务需要判断两个句子是否存在逻辑上的连续关系。这种关系不仅需要理解每个句子内部的语义，还需要结合两个句子的上下文信息进行全局建模。
  - Encoder 模式能够统一编码两句输入的全局上下文信息，而无需按生成顺序处理，极大提升了句子关系建模的能力。
---
### 4.应用场景：BERT 的主要应用都偏向自然语言理解，而非生成任务
- **分类任务**（如情感分析）：通过 [CLS] 位置的编码向量总结整个句子的语义。
- **序列标注任务**（如命名实体识别、词性标注）：需要对每个词进行精细化的语义理解，同时结合上下文来确定词的具体含义。
- **问答任务**：BERT 通过双向上下文编码，能够准确定位问题和文本中答案的关联关系。
- 相比之下，生成任务（如机器翻译、文本生成）更适合 Decoder 或 Encoder-Decoder 模式，因为它们需要通过序列化的方式逐步生成输出文本。BERT 的设计初衷是服务于理解任务，因此采用更契合的 Encoder 模式。

## BERT和GPT的区别
BERT（Bidirectional Encoder Representations from Transformers）和GPT（Generative Pretrained Transformer）是两种非常流行的自然语言处理（NLP）模型，虽然它们都基于Transformer架构，但它们的设计目标和使用方式有很大区别。
### 1. 模型架构
   - **BERT**：BERT 是一个 **编码器（Encoder）** 模型，采用了 **双向**（bidirectional）的训练方法。它的核心思想是通过上下文信息来预测文本中的单词，因此它同时考虑了单词的前后文。
   - **GPT**：GPT 是一个 **解码器（Decoder）** 模型，采用 **单向**（unidirectional）的训练方法，通常是从左到右的顺序生成文本。这意味着GPT只通过前文的信息来生成下一步的单词。
### 2. 训练目标
   - **BERT**：BERT的训练目标是 **掩码语言建模（Masked Language Modeling, MLM）**，即随机掩盖输入句子中的一些词，然后让模型预测这些被掩盖的词。这种方法使BERT能够从上下文中获得更多的信息，形成双向的理解。BERT 采用遮蔽语言模型（Masked Language Model，MLM） 与下一句预测（Next Sentence Prediction，NSP）作为训练目标。MLM会随机遮蔽输入文本中的一些单词，让模型预测被遮蔽的部分，这使它能深度理解文本语义；NSP任务则用于判断两句话在原文里是否相邻，增强对句子关系的捕捉。
   - **GPT**：GPT的训练目标是 **自回归语言建模（Autoregressive Language Modeling）**，也就是逐步预测下一个单词，每次都基于之前的单词进行预测。这使得GPT专注于生成文本。
### 3. 任务适应性
   - **BERT**：BERT主要用于 **下游任务**（downstream tasks），例如文本分类、命名实体识别、问答系统等。它通常需要通过微调（fine-tuning）来适应特定的任务。
   - **GPT**：GPT主要用于 **文本生成**（generation tasks），例如生成对话、文章写作等。此外，GPT模型可以通过“零样本学习”（zero-shot learning）或少样本学习（few-shot learning）进行适应，无需大量的任务特定微调。
### 4. 模型输入与输出
  - **BERT**：BERT的输入是经过处理的上下文，通常是两个句子（如问答任务中的问题和答案），并且其输出是句子的每个词的表示。输出可以是词级别的表示或句子级别的表示（如分类任务）。
  - **GPT**：GPT的输入是一个序列的文本，模型的输出是生成该序列的下一个单词。生成过程是自回归的，每次预测后会将预测的词作为下一步的输入，逐步生成文本。
### 5. 应用场景
  - **BERT**：更擅长自然语言理解任务，像是情感分析、命名实体识别，因为它双向理解文本的特性可以精准捕捉语义，给分类、抽取类任务提供稳固支撑。
  - **GPT**：在文本生成领域大放异彩，例如创意写作、机器翻译、对话生成，依靠自回归的模式源源不断“创作”新内容，维持文本连贯性。

## Encoder-Only（仅编码器）和Decoder-Only（仅解码器）的区别

- 在Transformer模型中，**Encoder-Only**和**Decoder-Only**是两种不同的架构模式。它们的区别主要体现在输入处理、模型任务和生成方式上。
---
### 1. 架构组成
- **Encoder-Only**：
  - 只有编码器部分，通常用于需要**理解输入数据**或生成某些特征表示的任务。
  - 编码器的任务是处理输入信息，将其转化为一个高维的表示（上下文向量）。

- **Decoder-Only**：
  - 只有解码器部分，通常用于基于某种上下文或输入生成输出的任务。
  - 解码器基于提供的上下文信息生成输出序列，常用于生成式任务，如文本生成或对话生成。
---
### 2. 任务和使用场景
- **Encoder-Only**：
  - 主要用于**特征提取**任务或理解任务，如**文本分类**、**情感分析**、**机器翻译中的编码部分**。
  - 任务是对输入进行编码并产生一个表示，这个表示可以用于后续的决策或分类。
  - 例如：在BERT模型中，只有编码器部分用于理解输入的上下文信息。

- **Decoder-Only**：
  - 主要用于**生成任务**，如**文本生成**、**语言模型**、**对话系统**。
  - 解码器通过生成单词或字符来**逐步生成输出**，每生成一个新的单词，都会用前面的内容和上下文来预测下一个单词。
  - 例如：GPT模型就是一个只包含解码器的架构，专注于生成文本。
---
### 3. 输入和输出
- **Encoder-Only**：
  - 输入：通常是固定长度的输入序列（如文本、图像等）。
  - 输出：生成一个高维的表示或上下文向量，通常用于后续的分类或回归任务。

- **Decoder-Only**：
  - 输入：通常是某种上下文或已经生成的部分输出（如给定一部分文本，解码器生成下一个单词）。
  - 输出：逐步生成输出序列，通常是文本、代码、图像描述等。
---
### 4. 处理方式
- **Encoder-Only**：
  - **自注意力机制**：编码器通常使用自注意力（self-attention）来理解输入中各个部分之间的关系。
  - 结果是一个对输入序列的全局理解，可以传递给下游任务（如分类器、回归模型等）。

- **Decoder-Only**：
  - **自回归生成**：解码器通过自回归的方式生成文本，逐步根据前文生成后续内容。
  - 每生成一个新的词，它只基于当前已经生成的部分和上下文信息来预测下一个词。
---
### 5. 模型应用
- **Encoder-Only**：
  - **BERT**：BERT模型采用了**仅编码器**的结构，它通过自注意力机制学习输入的上下文表示，适用于句子理解、分类任务等。

- **Decoder-Only**：
  - **GPT**：GPT模型采用了**仅解码器**的结构，专注于生成文本，通常是基于某种初始输入生成后续的语言输出。
---
### 6. 训练目标
- **Encoder-Only**：
  - 训练目标通常是学习如何从输入中提取有意义的表示。例如，BERT通过**掩蔽语言模型**（Masked Language Model，MLM）来训练，只关注如何理解输入。
  
- **Decoder-Only**：
  - 训练目标通常是逐步生成输出。例如，GPT通过**自回归语言模型**训练，模型基于前面的词预测下一个词。

## Encoder-Only和Decoder-Only中的attention有什么区别

- 在**Only Encoder（仅编码器）**和**Only Decoder（仅解码器）**的结构中，**Attention机制**的应用存在一些显著的不同，特别是在**自注意力**（Self-Attention）和**跨注意力**（Cross-Attention）方面的应用。以下是这两者中**Attention机制**的详细区别：

---

### **1. 仅编码器中的 Attention**
在仅编码器结构中，Attention机制主要用于理解和表示输入信息，模型关注的是如何在输入序列内部学习不同部分之间的关系。

#### **自注意力（Self-Attention）**：
- **作用**：自注意力用于输入序列的各个位置之间的信息交互，它使得每个位置的表示能够聚焦于整个输入序列的不同部分，从而学习到全局信息。
- **工作方式**：对于输入序列中的每一个词（或标记），通过计算该词与其他所有词的**相关性**来调整其表示。
- **计算流程**：
  - 输入的每个词都会计算自己与其他词的**注意力权重**。
  - 根据这些权重对输入的所有词进行加权求和，得到该词的新的表示。

#### **编码器结构的Attention特点**：
- **双向自注意力**：在标准的编码器中，每个位置的信息是可以与其他所有位置的信息交互的，形成一个全局的依赖关系。
- **并行处理**：因为自注意力机制不涉及时间顺序，所以在编码器中所有位置的注意力计算可以并行执行，这使得训练更加高效。

#### **典型模型**：
- **BERT**：采用了基于Transformer编码器的结构，只包含自注意力（Self-Attention）层，用于学习文本的上下文表示。

---

### **2. 仅解码器中的 Attention**
在仅解码器结构中，Attention机制的设计重点是生成任务的需求，解码器不仅需要关注输入的信息，还需要利用已经生成的部分信息来生成新的部分。

#### **自注意力（Self-Attention）**：
- **作用**：解码器中的自注意力机制不仅仅依赖于输入，还要依赖于已经生成的输出（例如，前面的单词），使得模型能够逐步生成内容。
- **工作方式**：解码器中的自注意力是“**掩蔽自注意力**”（Masked Self-Attention）。这是为了保证模型生成时的**自回归性质**，即每个位置只能关注到当前位置及之前生成的部分，不能看到未来的部分。

#### **跨注意力（Cross-Attention）**：
- **作用**：在解码器中，跨注意力用于将编码器的输出和解码器的输入结合起来，解码器通过这个机制从编码器中获取与当前解码位置相关的信息。
- **工作方式**：跨注意力通过计算解码器当前位置与编码器输出的**相关性**，从而调整解码器当前生成的词语。
- **计算流程**：
  - 解码器中的每个位置会根据自注意力生成一个新的表示，随后通过跨注意力与编码器的输出进行交互，从而融合输入序列的上下文信息。

#### **解码器结构的Attention特点**：
- **掩蔽自注意力**：解码器中的自注意力是“**掩蔽**”的，只允许每个词位置依赖于当前生成的词和前面的词，不能看到未来的词，这保证了解码过程的自回归生成特性。
- **跨注意力**：解码器会与编码器的输出进行交互，提取编码器生成的上下文信息，帮助解码器生成更加准确的输出。

#### **典型模型**：
- **GPT**：采用了仅解码器的结构，主要使用掩蔽自注意力来逐步生成输出。
- **Transformer**：在机器翻译任务中，标准的Transformer模型中包含了编码器和解码器，解码器中有自注意力和跨注意力。

---

### **3. Attention的主要区别**

| 特性                 | 仅编码器中的Attention                                    | 仅解码器中的Attention                                                                  |
| -------------------- | -------------------------------------------------------- | -------------------------------------------------------------------------------------- |
| **自注意力**         | 自注意力用于输入序列内部的信息交互，所有位置可以互相注意 | 自注意力用于生成过程中的部分内容（掩蔽自注意力），只允许当前位置及之前的部分进行交互   |
| **跨注意力**         | 不涉及跨注意力，只有自注意力                             | 通过跨注意力从编码器获取上下文信息，辅助生成输出                                       |
| **注意力机制的类型** | **自注意力**：每个位置与其他所有位置之间计算相关性       | **掩蔽自注意力**（Masked Self-Attention）与**跨注意力**（Cross-Attention）相结合       |
| **计算方式**         | 对于输入序列中的每个位置，计算该位置与所有位置的关系     | 对于解码过程中的每个位置，计算当前位置与之前的所有位置的关系，同时与编码器输出进行交互 |
| **生成任务中的角色** | 用于理解和表示输入的语义                                 | 用于生成任务，通过自回归生成输出并参考编码器的信息                                     |

## BERT是怎么样预测下一句的⭐
BERT的"预测下一句"（Next Sentence Prediction, NSP）任务是其预训练任务之一，目的是帮助模型理解句子之间的关系。具体来说，BERT在预训练阶段会使用如下方式来进行下一句预测：

### 1. 数据准备
在训练过程中，BERT使用一个句对（Sentence A 和 Sentence B）。其中：
- **正例**（positive example）：Sentence B是紧接在Sentence A之后的句子。
- **负例**（negative example）：Sentence B是随机从其他句子中选取的，不与Sentence A有直接关系。

### 2. 任务目标
BERT的目标是根据给定的句对，预测Sentence B是否是Sentence A的下一句。模型通过分类任务来判断这个关系。这个任务使用了一个**二分类**（binary classification）策略：
- 如果Sentence B确实是Sentence A的下一句，则标签为1（正例）。
- 如果Sentence B不是Sentence A的下一句，则标签为0（负例）。

### 3. 训练过程
BERT通过Transformer架构将Sentence A和Sentence B的表示进行编码。在模型输入时，两个句子被合并为一个序列，并通过特殊的token标记来表示：
- `[CLS]`：句子对的开始标记（用于分类任务）。
- `[SEP]`：句子分隔符，用于标记Sentence A和Sentence B的边界。

然后，BERT通过其双向编码器（Transformer）来学习上下文关系，并为每个输入句对生成一个表示。最终，模型将对句子对的关系进行预测。

### 4. 预测过程
在预测阶段，BERT会给定一个句子对（例如，A和B），并计算输出的`[CLS]` token的表示。然后，这个表示会通过一个分类层来预测Sentence B是否为Sentence A的下一句。

### 5. 输出结果
根据分类层的输出，BERT会预测下一句的概率。如果预测结果是1，表示Sentence B是Sentence A的下一句；如果是0，表示不是。
# RoBERTa
继BERT、XLNet之后，Facebook提出的RoBERTa（a Robustly Optimized BERT Pretraining Approach）。RoBERTa在模型结构层面没有改变，改变的只是预训练的方法，具体是以下三点：

## 1.动态mask
BERT采用静态mask，随机选择15%的tokens替换，为了消除上下游任务的不匹配问题，对这15%的tokens进行：(1)80%的时间替换为[MASK] （2）10%的时间不变 （3）10%的时间替换为其他词

RoBERTa把预训练的数据复制10份，每一份都随机选择15%的Tokens进行mask，也就是说，同样的一句话有10种不同的mask方式。然后每份数据都训练N/10个epoch。这就相当于在这N个epoch的训练中，每个序列的被mask的tokens是会变化的。
## 2.去掉NSP任务
BERT使用了NSP任务，对于输入的两个句子判断是否为连续的。训练数据的构成为，50%的样本是同一篇文章的上下句，50%的样本是非上下文的两句话。

RoBERTa去掉了NSP任务，使用FULL-SENTENCES训练方式。每次输入连续的多个句子，直到最大长度512。

## 3.更大的mini-batch，更多的数ff据
- RoBERTa使用的batch size为8k，BERT使用的batch size是256。
- RoBERTa使用的数据量为160G，BERT使用的数据量为13G